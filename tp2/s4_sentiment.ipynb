{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "## 1. Textblob-FR\n",
    "\n",
    "Documentation: https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une fonction `get_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser le sentiment d'une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 20% negative and 0.3% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Les textes prévoient expressément l’utilisation de la préférence pour le cas où la Belgique entendrait aliéner les territoires, les échanger, les concéder, les louer, à une puissance étrangère.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 10% positive and perfectly objective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Conformément à l’article 73 de la Charte des Nations Unies, il amène le peuple congolais a l’indépendance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 3% positive and 0.06666666666666667% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"LA SUCCESSION D'EISENHOWER PETITE GAZETTE M. Nixon devra compter avec le sénateur Kennedy FEUX VERTS La Belgique va, enfin! avoir une politique scientifique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 42% positive and 0.46666666666666673% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"LE som VENDREDI 11 MARS 1960 ÇA, LA BELLE EPOQUE ? Ce qui est sûr, c'est que ce n’était pas une époque pratique !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is neutral and perfectly objective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Pourquoi le relèvement du plafond, au lieu d’une augmentation du pourcentage des cotisations?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 10% positive and 0.4% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Et encore cette image poétique. : « la laine de couleur est pareille à une musique ».\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 40% negative and 0.4% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Il gardait ce recueil sur sa table de chevet et disait à J. J. Brousson. son secrétaire : — Quand Je me sens las, je le feuillette.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is neutral and perfectly objective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Le bourreau fait 5 moins le martyr que le martyr s ne fait le bourreau.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is 50% negative and 0.2% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"Etrange domaine que cette suite de granges et d’escaliers où la bohème montoise s’est réfugiée I Pourquoi ne pas l’avouer ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is neutral and 0.1% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"En effet, nous l’avons vu, des déficiences circulatoires ont une éventuelle incidence sur l’apparition des vertiges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilisation de transformers\n",
    "\n",
    "Documentation: https://github.com/TheophileBlard/french-sentiment-analysis-with-bert\n",
    "\n",
    "**!!** Si le code ne tourne pas sur votre machine, vous pouvez le tester directement sur Google Colab en utilisant [ce lien](https://colab.research.google.com/github/TheophileBlard/french-sentiment-analysis-with-bert/blob/master/colab/french_sentiment_analysis_with_bert.ipynb) **!!**\n",
    "\n",
    "Le modèle peut également être testé en ligne sur [HuggingFace](https://huggingface.co/tblard/tf-allocine)\n",
    "\n",
    "### Installation des librairies et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/27/7e/9699c0ebd2bf124df8446bc4767390c023f7bb2d072b7ff92ae7f3fc1379/tensorflow-2.14.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp39-cp39-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.14.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.14.0 from https://files.pythonhosted.org/packages/29/ab/79512973474b56a7f9efef2cab0ec2f803759e90e1b65ff0383d2b0788c4/tensorflow_intel-2.14.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.14.0-cp39-cp39-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/98/e0/679ec9293f69e28406215d2209a949c7e615d459fdee90103e8bb0d75c41/h5py-3.10.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/c7/47/54b1e5eea9ed7f8a5f701713e47ea45e798a4f3e5f476a053fd0b537e2af/ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.26.0)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/5c/75/ee4333e73b938bad975bbc3f7e3f12cbb37baaf53c411c959360b5a8e352/protobuf-4.25.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.0-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (68.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.12.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.8.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/a3/45/d69aedc11317d30840afd14f64cd47ff62d74a6f88c7fc5fda502d4623fd/grpcio-1.59.2-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.59.2-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/86/a7/75911c13a242735d5aeaca6a272da380335ff4ba5f26d6b2ae20ff682d13/google_auth-2.23.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/70/58/2c5a654173937d9f540a4971c569b44dcd55e5424a484d954cdaeebcf79c/Markdown-3.5.1-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.3/181.3 kB 88.3 kB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.17.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.9/83.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.14.0-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.14.0-cp39-cp39-win_amd64.whl (284.1 MB)\n",
      "   ------------------------                 173.8/284.1 MB 3.2 MB/s eta 0:00:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 248, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 161, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 565, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 479, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 587, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     ------------------------------------ 977.6/977.6 kB 672.7 kB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/9a/06/e4ec2a321e57c03b7e9345d709d554a52c33760e5015fdff0919d9459af0/transformers-4.35.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "     -------------------------------------- 123.1/123.1 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/84/4d/82704d1ab9290b03da94e6425f5e87396b999fd7eb8e08f3a92c158402bf/PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/36/de/de1b1d7b191821cc2e6e84251cf9641e4fbd205fa5ec816d52fe42f97325/tokenizers-0.14.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/a7/49/d43e967d427e78086980ec85af1afe0320becfc33b26903d3d0ce10b0ee5/safetensors-0.4.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\documents\\ma stic\\ma stic ii\\traitement automatique de corpus\\2023-2024\\tds\\tac\\tac_venv\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 7.9/7.9 MB 108.1 kB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 152.8/152.8 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp39-none-win_amd64.whl (277 kB)\n",
      "   --------------------------------------- 277.2/277.2 kB 71.5 kB/s eta 0:00:00\n",
      "Downloading tokenizers-0.14.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 2.2/2.2 MB 66.4 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "   -------------------------------------- 295.0/295.0 kB 102.4 kB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.17.3 pyyaml-6.0.1 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 118B/s]\n",
      "c:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 666/666 [00:00<00:00, 95.2kB/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 811k/811k [00:10<00:00, 75.3kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 210/210 [00:00<00:00, 21.0kB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nCamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tp2\\s4_sentiment.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Ma%20STIC/Ma%20STIC%20II/Traitement%20Automatique%20de%20Corpus/2023-2024/TDs/tac/tp2/s4_sentiment.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mtblard/tf-allocine\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_pt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Ma%20STIC/Ma%20STIC%20II/Traitement%20Automatique%20de%20Corpus/2023-2024/TDs/tac/tp2/s4_sentiment.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m TFAutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mtblard/tf-allocine\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Ma%20STIC/Ma%20STIC%20II/Traitement%20Automatique%20de%20Corpus/2023-2024/TDs/tac/tp2/s4_sentiment.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sentiment_analyser \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39msentiment-analysis\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:786\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[0;32m    785\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 786\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    788\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2024\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2021\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2022\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2024\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2025\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2026\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2027\u001b[0m     init_configuration,\n\u001b[0;32m   2028\u001b[0m     \u001b[39m*\u001b[39minit_inputs,\n\u001b[0;32m   2029\u001b[0m     token\u001b[39m=\u001b[39mtoken,\n\u001b[0;32m   2030\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2031\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2032\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2033\u001b[0m     _is_local\u001b[39m=\u001b[39mis_local,\n\u001b[0;32m   2034\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2035\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2254\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2256\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs)\n\u001b[0;32m   2257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   2258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   2259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2260\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\models\\camembert\\tokenization_camembert_fast.py:127\u001b[0m, in \u001b[0;36mCamembertTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    112\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    113\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m ):\n\u001b[0;32m    125\u001b[0m     \u001b[39m# Mask token behave like a normal word, i.e. include the space before it. Will have normalized = False\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, special\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[1;32m--> 127\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    128\u001b[0m         vocab_file,\n\u001b[0;32m    129\u001b[0m         tokenizer_file\u001b[39m=\u001b[39mtokenizer_file,\n\u001b[0;32m    130\u001b[0m         bos_token\u001b[39m=\u001b[39mbos_token,\n\u001b[0;32m    131\u001b[0m         eos_token\u001b[39m=\u001b[39meos_token,\n\u001b[0;32m    132\u001b[0m         sep_token\u001b[39m=\u001b[39msep_token,\n\u001b[0;32m    133\u001b[0m         cls_token\u001b[39m=\u001b[39mcls_token,\n\u001b[0;32m    134\u001b[0m         unk_token\u001b[39m=\u001b[39munk_token,\n\u001b[0;32m    135\u001b[0m         pad_token\u001b[39m=\u001b[39mpad_token,\n\u001b[0;32m    136\u001b[0m         mask_token\u001b[39m=\u001b[39mmask_token,\n\u001b[0;32m    137\u001b[0m         additional_special_tokens\u001b[39m=\u001b[39madditional_special_tokens,\n\u001b[0;32m    138\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[0;32m    112\u001b[0m \u001b[39melif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[39m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1344\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1337\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn instance of tokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_name\u001b[39m}\u001b[39;00m\u001b[39m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1339\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1340\u001b[0m     )\n\u001b[0;32m   1342\u001b[0m converter_class \u001b[39m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[1;32m-> 1344\u001b[0m \u001b[39mreturn\u001b[39;00m converter_class(transformer_tokenizer)\u001b[39m.\u001b[39mconverted()\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:459\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m--> 459\u001b[0m     requires_backends(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mprotobuf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    461\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[0;32m    463\u001b[0m     \u001b[39m# from .utils import sentencepiece_model_pb2 as model_pb2\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Ma STIC\\Ma STIC II\\Traitement Automatique de Corpus\\2023-2024\\TDs\\tac\\tac_venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1239\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1237\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[0;32m   1238\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m-> 1239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nCamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\", use_pt=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "\n",
    "sentiment_analyser = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser le sentiment d'une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Ce journal est vraiment super intéressant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Cette phrase est négative et je ne suis pas content !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
